{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Cooking with NLP part 2\n",
    "\n",
    "Hello again! Following our first attempt where we used 700 recipes from scraped data, we'll now try again with a database containing 30,000 recipes as well as also exploring Doc2Vec from gensim and see how well it compares with our previously seen methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alexkeenan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "import re\n",
    "import zipfile\n",
    "import io\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from myscripts import *\n",
    "import gensim\n",
    "import logging\n",
    "import pprint\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 6)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "data_folder=\"../data/\"\n",
    "#@@@@@ Now You're Cooking! Export Format\n",
    "\n",
    "use_old_models=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded preprocessed df\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    1/use_old_models # if use_old_models=0, then this fails\n",
    "    cols_to_use=[\"corpus\",\"name\"]\n",
    "    #for some reason when you load, it creates another index col that it tacks for no reason...\n",
    "    combined_df=pd.read_csv(os.path.join(data_folder,\"processed_all_recipes.csv\"),encoding=\"ISO-8859-1\",usecols=cols_to_use)\n",
    "    \n",
    "    #has columns  [\"corpus\",\"name\"]\n",
    "    print(\"loaded preprocessed df\")\n",
    "except:\n",
    "    print(\"new text preprocessing\")\n",
    "    recipes_df=pd.read_csv(os.path.join(data_folder,\"all_recipes.csv\"),encoding=\"ISO-8859-1\")\n",
    "    recipes_df.dropna(inplace=True)\n",
    "    combined_df=pd.DataFrame()\n",
    "    #combined_df[\"corpus\"] = recipes_df[\"ingredients\"].map(str) +\" \"+recipes_df[\"instructions\"]+\" \"+recipes_df[\"name\"]\n",
    "    combined_df[\"corpus\"] = recipes_df[\"ingredients\"].map(str) +\" \"+recipes_df[\"name\"]\n",
    "    combined_df[\"name\"]=recipes_df[\"name\"]\n",
    "    \n",
    "    combined_df=combined_df.sample(frac=1)\n",
    "    combined_df=combined_df.reset_index(drop=True)\n",
    "\n",
    "    combined_df.corpus=combined_df.corpus.apply(lambda x:\" \".join(text_process(str(x)))) #apparently sometimes x is a float, that's why str() is there\n",
    "    combined_df.to_csv(os.path.join(data_folder,\"processed_all_recipes.csv\"))\n",
    "    print(\"done preprocessing\")\n",
    "\n",
    "# Turning text into tagged doc2vec objects\n",
    "        \n",
    "train_test_dict=train_test(0.9,x=combined_df)\n",
    "x_train=train_test_dict[\"x_train\"]\n",
    "x_test=train_test_dict[\"x_test\"]\n",
    "\n",
    "def processing_train_test_text(train_or_test_df):\n",
    "    list_of_processed_texts=[]\n",
    "    for each in train_or_test_df.corpus.tolist():\n",
    "        list_of_processed_texts.append(str(each).split())\n",
    "    return list_of_processed_texts\n",
    "\n",
    "train_processed_tokenized_text=processing_train_test_text(x_train)\n",
    "test_processed_tokenized_text=processing_train_test_text(x_train)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "LDA and LSI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import struct\n",
    "from gensim import corpora\n",
    "\n",
    "num_topics=20\n",
    "\n",
    "\n",
    "data_folder=\"../data/\"\n",
    "#THE WHOLE PREPROCESSING CHECK THING COULD'VE BEEN DONE EARLIER in the script\n",
    "#i save my lemmatized words as bin files, to save space + special characters\n",
    "try:\n",
    "    1/use_old_models # if use_old_models=0, then this fails\n",
    "    my_nlp_dict=corpora.Dictionary.load(os.path.join(data_folder, 'my_nlp_'+str(num_topics)+'.dict'))\n",
    "    #with open(os.path.join(data_folder,\"lemmatized_processed.bin\"), \"rb\") as my_input:  \n",
    "    #    fileContent=my_input.read()\n",
    "    #    train_processed_tokenized_text = exec(fileContent.decode('utf-8'))\n",
    "    train_processed_tokenized_text=np.load(os.path.join(data_folder,'train_processed_tokenized_text.npy')).tolist()\n",
    "\n",
    "except:\n",
    "    #with open(os.path.join(data_folder,\"lemmatized_processed.bin\"), \"wb\") as output:\n",
    "    #    output.write(str(train_processed_tokenized_text).encode('utf-8'))\n",
    "    np.save(os.path.join(data_folder,\"train_processed_tokenized_text\"),train_processed_tokenized_text)\n",
    "    my_nlp_dict = corpora.Dictionary(train_processed_tokenized_text)\n",
    "    my_nlp_dict.filter_extremes(no_below=5, no_above=0.2) #in case you want to filter out some words\n",
    "    my_nlp_dict.save(os.path.join(data_folder, 'my_nlp_'+str(num_topics)+'.dict'))  # store the dictionary, for future reference\n",
    "    my_nlp_dict=my_nlp_dict.load(os.path.join(data_folder, 'my_nlp_'+str(num_topics)+'.dict'))\n",
    "    #print(my_nlp_dict)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Making the BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#from gensim.corpora import Dictionary\n",
    "#dictionary = Dictionary(train_texts)\n",
    "\n",
    "try:\n",
    "    1/use_old_models # if use_old_models=0, then this fails\n",
    "    bow_corpus = corpora.MmCorpus(os.path.join(data_folder, 'MY_BOW.mm'))\n",
    "    \n",
    "except:\n",
    "    print(\"creating BOW\")\n",
    "    #STRAIGHT FROM TEXT (COULD BE MEMORY INTENSIVE)\n",
    "    \n",
    "    bow_corpus = [my_nlp_dict.doc2bow(text) for text in train_processed_tokenized_text]\n",
    "    corpora.MmCorpus.serialize(os.path.join(data_folder, 'MY_BOW.mm'), bow_corpus)  # store to disk, for later use\n",
    "    bow_corpus = corpora.MmCorpus(os.path.join(data_folder, 'MY_BOW.mm')) \n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# BOW -> TFIDF -> LSI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using old LSI model\n",
      "using old TFIDF model\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    1/use_old_models # if use_old_models=0, then this fails\n",
    "    lsi_model = gensim.models.LsiModel.load(os.path.join(data_folder,'lsi_'+str(num_topics)+'.model'))\n",
    "    print(\"using old LSI model\")\n",
    "    tfidf_model=gensim.models.TfidfModel.load(os.path.join(data_folder,'Tfidf_'+str(num_topics)+'.model'))\n",
    "    print(\"using old TFIDF model\")\n",
    "    corpus_tfidf = tfidf_model[bow_corpus]\n",
    "except:\n",
    "    print(\"creating new tfidf_model\")\n",
    "    tfidf_model = gensim.models.TfidfModel(bow_corpus) \n",
    "    \n",
    "    corpus_tfidf = tfidf_model[bow_corpus]\n",
    "    print(\"creating new LSI model\")\n",
    "    %time lsi_model = gensim.models.LsiModel(corpus_tfidf, id2word=my_nlp_dict, num_topics=num_topics) # initialize an LSI transformation\n",
    "    lsi_model.save(os.path.join(data_folder,'lsi_'+str(num_topics)+'.model'))\n",
    "    tfidf_model.save(os.path.join(data_folder,'Tfidf_'+str(num_topics)+'.model'))\n",
    "    print(\"done\")\n",
    "    #lda = gensim.models.ldamodel.LdaModel(model_corpus, num_topics = n_topics_lda, id2word = id2word, passes = n_passes_lda)\n",
    "    #gensim.models.ldamodel.LdaModel.save(lda, 'outputs/lda.model')\n",
    "lsi_corpus = lsi_model[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Similarities matrix built on LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading similarities matrix\n"
     ]
    }
   ],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "try:\n",
    "    1/use_old_models \n",
    "    index = similarities.MatrixSimilarity.load(os.path.join(data_folder, 'lsi_similarity_'+str(num_topics)+'.index'))\n",
    "    print(\"loading similarities matrix\")\n",
    "except:\n",
    "    print(\"creating new similarities matrix\")\n",
    "    index = similarities.MatrixSimilarity(lsi_corpus)\n",
    "    index.save(os.path.join(data_folder, 'lsi_similarity_'+str(num_topics)+'.index'))\n",
    "    index = similarities.MatrixSimilarity.load(os.path.join(data_folder, 'lsi_similarity_'+str(num_topics)+'.index'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Can it recognize itself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target recipe at index 0\n",
      "Rumaki \n",
      "INDEXES\n",
      "[0, 5114, 15664, 4685, 6991, 4267, 17732, 10103, 28528, 23918, 10901, 4630, 24406, 13722, 6770, 28099, 28052, 12676, 14801, 21106]\n",
      "RECIPES\n",
      "[   ('Rumaki ', 0.96698177),\n",
      "    ('Oriental Grilled Chicken ', 0.95149982),\n",
      "    ('Teriyaki Chicken Delight ', 0.94270492),\n",
      "    ('Tofu Triangles ', 0.93615127),\n",
      "    ('Hot & Spicy Chicken ', 0.93612719),\n",
      "    ('Chinese: Hot & Spicy Chicken (Ma La Tze Gee ', 0.93601215),\n",
      "    ('Chinese: Hot And Spicy Chicken (Ma La Tze Gee) - Hunan ', 0.93601215),\n",
      "    ('Chinese Braised Chicken ', 0.93278378),\n",
      "    ('Crockpot Cowloon Chicken ', 0.93268424),\n",
      "    ('Baked Teriyaki Whiskey Chicken ', 0.93097496),\n",
      "    ('Mandarin Soup ', 0.92793053),\n",
      "    ('Roasted Spare Ribs (Pei Quot) ', 0.92731059),\n",
      "    ('Crockpot Kowloon Chicken ', 0.92438132),\n",
      "    ('Yunnan Steamed Pot Chicken ', 0.92310202),\n",
      "    ('Won-Ton Soup ', 0.92039132),\n",
      "    ('Salt-Roasted Chicken With Marinade ', 0.91991341),\n",
      "    ('Oriental Chicken Marinade ', 0.91613233),\n",
      "    ('Mushroom Soup - Chinese ', 0.91487527),\n",
      "    ('Grilled Japanese Chicken ', 0.91480494),\n",
      "    ('Sour Soup With Rice Noodles ', 0.91436779)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_run=x_train.corpus[0] #picking the first recipe\n",
    "\n",
    "vec_bow = my_nlp_dict.doc2bow(test_run.split())\n",
    "#print(vec_bow) #this part works\n",
    "vec_tfidf=tfidf_model[vec_bow]\n",
    "#print(vec_tfidf) # you join it with the model, not the corpus\n",
    "vec_lsi = lsi_model[vec_tfidf] # convert the query to LSI space\n",
    "sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "#print(list(enumerate(sims))) # print (document_number, document_similarity) 2-tuples\n",
    "\n",
    "top_10=list(sorted(enumerate(sims), key=lambda x: x[1], reverse=True))[:20]\n",
    "top_10_indexes=[x[0] for x in top_10]\n",
    "top_10_match_pct=[x[1] for x in top_10]\n",
    "print(\"Target recipe at index 0\")\n",
    "print(x_train.name[0])\n",
    "print(\"INDEXES\")\n",
    "print(top_10_indexes)\n",
    "top_10_names=x_train.name[top_10_indexes].values\n",
    "\n",
    "print(\"RECIPES\")\n",
    "pp.pprint(list(zip(top_10_names,top_10_match_pct)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yes it can.\n",
    "As you can see above, the selected target recipe was Rumaki with a very high similarity score. This is no big surprise but it tells us the model is working correctly. (For the curious, Rumaki is just cocktail sausages wrapped in bacon.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Making a request\n",
    "\n",
    "Like in my previous post, let's see how well it can handle vague requests. Feel free to try this out yourself!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEXES\n",
      "[23888, 8732, 27949, 19665, 15911, 8008, 11508, 1388, 24574, 20455, 12667, 28763, 28755, 24526, 846, 4045, 10823, 12663, 12848, 15279]\n",
      "RECIPES\n",
      "[   ('Barvarian Sausage Salad ', 0.92748308),\n",
      "    ('Bayerischer Wurstsalat (Barvarian Sausage Salad) ', 0.9272368),\n",
      "    ('Bayerischer Wurstsalat (Barvarian Sausage Sala ', 0.90767419),\n",
      "    ('Bayerischer Wurstsalat (Barvarian Sausage Sal ', 0.90714264),\n",
      "    ('Cajun Coleslaw ', 0.88668299),\n",
      "    ('Lobster Salad ', 0.87653732),\n",
      "    ('Lobster Salad ', 0.87653732),\n",
      "    ('Avocado-Tofu-Egg Dip ', 0.87410486),\n",
      "    ('Avocado Tofu Egg Dip ', 0.87410486),\n",
      "    ('Sweet Salad Dressing Mix ', 0.86581641),\n",
      "    ('Cajun Potato Salad ', 0.86413437),\n",
      "    ('Turkey Salad Stuffed Eggs ', 0.85851169),\n",
      "    (\"Justin Wilson's Tuna Salad \", 0.85167211),\n",
      "    (\"Justin's Tuna Salad \", 0.85074282),\n",
      "    ('Eggplant Relish ', 0.8463912),\n",
      "    ('Italian Slaw ', 0.84473038),\n",
      "    ('Thousand Island Dressing ', 0.8444435),\n",
      "    ('Potato Salad Dressing ', 0.84361196),\n",
      "    ('Spam Shake ', 0.84287912),\n",
      "    ('Smoked Fish Spread ', 0.84248132)]\n"
     ]
    }
   ],
   "source": [
    "test_run='spicy meat salad'\n",
    "\n",
    "vec_bow = my_nlp_dict.doc2bow(test_run.split())\n",
    "#print(vec_bow) #this part works\n",
    "vec_tfidf=tfidf_model[vec_bow]\n",
    "#print(vec_tfidf) # you join it with the model, not the corpus\n",
    "vec_lsi = lsi_model[vec_tfidf] # convert the query to LSI space\n",
    "sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "#print(list(enumerate(sims))) # print (document_number, document_similarity) 2-tuples\n",
    "\n",
    "top_10=list(sorted(enumerate(sims), key=lambda x: x[1], reverse=True))[:20]\n",
    "top_10_indexes=[x[0] for x in top_10]\n",
    "top_10_match_pct=[x[1] for x in top_10]\n",
    "print(\"INDEXES\")\n",
    "print(top_10_indexes)\n",
    "top_10_names=x_train.name[top_10_indexes].values\n",
    "\n",
    "print(\"RECIPES\")\n",
    "pp.pprint(list(zip(top_10_names,top_10_match_pct)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I asked for 'spicy meat salad' and the best suggestions include Barvarian sausage salad, Cajun Coleslaw, Lobster salad, etc. I'd say the suggestions are spot on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which topic does it resemble most? What words best represent those topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic numbers are\n",
      "(31, 33, 0, 19, 40, 16, 42, 1, 25, 38)\n",
      "----\n",
      "0.210*\"rice\" + -0.209*\"ginger\" + 0.204*\"chicken\" + 0.185*\"mustard\" + -0.184*\"soy\" + 0.179*\"steak\" + -0.174*\"fish\" + -0.172*\"carrot\" + -0.171*\"pineapple\" + 0.165*\"cook\"\n",
      "----\n",
      "-0.252*\"shrimp\" + -0.211*\"half\" + 0.207*\"rice\" + -0.194*\"gm\" + -0.183*\"banana\" + -0.176*\"ounce\" + 0.172*\"red\" + 0.152*\"almond\" + -0.149*\"pecan\" + 0.143*\"chip\"\n",
      "----\n",
      "0.162*\"ground\" + 0.153*\"sauce\" + 0.146*\"slice\" + 0.142*\"beef\" + 0.141*\"chocolate\" + 0.139*\"cheese\" + 0.139*\"bake\" + 0.138*\"fresh\" + 0.132*\"tomato\" + 0.126*\"mince\"\n",
      "----\n",
      "-0.402*\"shrimp\" + -0.254*\"banana\" + 0.251*\"corn\" + -0.231*\"rice\" + 0.178*\"honey\" + 0.171*\"wheat\" + 0.166*\"chicken\" + 0.162*\"steak\" + -0.149*\"bread\" + -0.148*\"cook\"\n",
      "----\n",
      "-0.358*\"pineapple\" + 0.262*\"roast\" + -0.252*\"dice\" + 0.251*\"ingredient\" + 0.196*\"pecan\" + 0.168*\"potato\" + 0.158*\"melt\" + 0.155*\"slice\" + -0.153*\"cherry\" + 0.149*\"strawberry\"\n",
      "----\n",
      "-0.350*\"potato\" + 0.348*\"peanut\" + -0.231*\"corn\" + 0.217*\"apple\" + -0.177*\"dice\" + -0.170*\"mustard\" + -0.164*\"sauce\" + -0.153*\"pork\" + 0.149*\"steak\" + -0.147*\"pecan\"\n",
      "----\n",
      "0.360*\"salmon\" + -0.308*\"finely\" + 0.212*\"shrimp\" + -0.167*\"walnut\" + -0.165*\"sourdough\" + 0.139*\"cheese\" + 0.138*\"seed\" + 0.137*\"mince\" + -0.133*\"dash\" + 0.133*\"beef\"\n",
      "----\n",
      "-0.364*\"chocolate\" + -0.247*\"bake\" + -0.245*\"vanilla\" + -0.173*\"cake\" + -0.172*\"soda\" + -0.148*\"chip\" + -0.141*\"extract\" + 0.134*\"beef\" + 0.125*\"tomato\" + -0.124*\"cocoa\"\n",
      "----\n",
      "-0.248*\"corn\" + -0.241*\"strawberry\" + -0.204*\"pkg\" + 0.202*\"bean\" + 0.199*\"bread\" + 0.196*\"extract\" + 0.190*\"steak\" + -0.188*\"mix\" + 0.171*\"pie\" + -0.155*\"gm\"\n",
      "----\n",
      "0.242*\"ham\" + 0.229*\"honey\" + -0.225*\"half\" + 0.196*\"gm\" + -0.182*\"white\" + 0.173*\"slice\" + -0.170*\"wheat\" + -0.158*\"ingredient\" + 0.153*\"mustard\" + 0.150*\"strawberry\"\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "def check_topics_words(model,model_vec,nber_words=10,nber_topics=10):\n",
    "    #getting the top nber_words most significant ones\n",
    "    model_vec=sorted(vec_lsi, key=lambda x: abs(x[1]),reverse=True)[:nber_topics]\n",
    "\n",
    "    topic_numbers, topic_similarities = zip(*model_vec)\n",
    "    print(\"topic numbers are\")\n",
    "    print(topic_numbers)\n",
    "    print(\"----\")\n",
    "    \n",
    "\n",
    "    for each_topic_number in topic_numbers:\n",
    "        print(model.print_topic(topicno=each_topic_number,topn=nber_words))\n",
    "        print(\"----\")\n",
    "        \n",
    "\n",
    "check_topics_words(lsi_model,vec_lsi,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the model groups certain ingredients together won't always make sense to us. In reality if this were a serious project used commercially, the number of subjects would either be set based on what we already know about the data, or tweaked until we felt the groups made sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try increasing the number of topics from 20 to 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using old LSI model\n",
      "using old TFIDF model\n",
      "loading similarities matrix\n"
     ]
    }
   ],
   "source": [
    "num_topics=50\n",
    "\n",
    "try:\n",
    "    1/use_old_models # if use_old_models=0, then this fails\n",
    "    lsi_model = gensim.models.LsiModel.load(os.path.join(data_folder,'lsi_'+str(num_topics)+'.model'))\n",
    "    print(\"using old LSI model\")\n",
    "    tfidf_model=gensim.models.TfidfModel.load(os.path.join(data_folder,'Tfidf_'+str(num_topics)+'.model'))\n",
    "    print(\"using old TFIDF model\")\n",
    "    corpus_tfidf = tfidf_model[bow_corpus]\n",
    "except:\n",
    "    print(\"creating new tfidf_model\")\n",
    "    tfidf_model = gensim.models.TfidfModel(bow_corpus) \n",
    "    \n",
    "    corpus_tfidf = tfidf_model[bow_corpus]\n",
    "    print(\"creating new LSI model\")\n",
    "    %time lsi_model = gensim.models.LsiModel(corpus_tfidf, id2word=my_nlp_dict, num_topics=num_topics) # initialize an LSI transformation\n",
    "    lsi_model.save(os.path.join(data_folder,'lsi_'+str(num_topics)+'.model'))\n",
    "    tfidf_model.save(os.path.join(data_folder,'Tfidf_'+str(num_topics)+'.model'))\n",
    "    print(\"done\")\n",
    "    #lda = gensim.models.ldamodel.LdaModel(model_corpus, num_topics = n_topics_lda, id2word = id2word, passes = n_passes_lda)\n",
    "    #gensim.models.ldamodel.LdaModel.save(lda, 'outputs/lda.model')\n",
    "lsi_corpus = lsi_model[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "\n",
    "try:\n",
    "    1/use_old_models \n",
    "    index = similarities.MatrixSimilarity.load(os.path.join(data_folder, 'lsi_similarity_'+str(num_topics)+'.index'))\n",
    "    print(\"loading similarities matrix\")\n",
    "except:\n",
    "    print(\"creating new similarities matrix\")\n",
    "    index = similarities.MatrixSimilarity(lsi_corpus)\n",
    "    index.save(os.path.join(data_folder, 'lsi_similarity_'+str(num_topics)+'.index'))\n",
    "    index = similarities.MatrixSimilarity.load(os.path.join(data_folder, 'lsi_similarity_'+str(num_topics)+'.index'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEXES\n",
      "[8732, 23888, 28763, 28755, 12663, 24526, 9192, 7254, 27949, 19665, 13584, 25704, 3959, 4718, 6910, 11895, 16104, 13424, 26181, 29574]\n",
      "RECIPES\n",
      "[   ('Bayerischer Wurstsalat (Barvarian Sausage Salad) ', 0.8107602),\n",
      "    ('Barvarian Sausage Salad ', 0.81046391),\n",
      "    ('Turkey Salad Stuffed Eggs ', 0.8019563),\n",
      "    (\"Justin Wilson's Tuna Salad \", 0.78364539),\n",
      "    ('Potato Salad Dressing ', 0.78361166),\n",
      "    (\"Justin's Tuna Salad \", 0.78264034),\n",
      "    (\"Bonnie's Potato Salad \", 0.77218527),\n",
      "    (\"Joyce's Easy Deviled Eggs \", 0.74774295),\n",
      "    ('Bayerischer Wurstsalat (Barvarian Sausage Sala ', 0.74241978),\n",
      "    ('Bayerischer Wurstsalat (Barvarian Sausage Sal ', 0.74213636),\n",
      "    ('Cavalier Dressing ', 0.73880136),\n",
      "    ('Deluxe Potato Salad ', 0.73818612),\n",
      "    ('Basil Potato & Egg Salad ', 0.73207104),\n",
      "    ('Basil, Potato And Egg Salad ', 0.73207104),\n",
      "    ('Basil, Potato And Egg Salad ', 0.73207104),\n",
      "    ('Basil Potato & Egg Salad ', 0.73207104),\n",
      "    ('Potato Salad (Prodigy) ', 0.73139638),\n",
      "    ('Roasted Potato Surimi Salad ', 0.73114282),\n",
      "    ('Tuna-Roni Salad ', 0.72792923),\n",
      "    ('Indian Salad ', 0.72537398)]\n"
     ]
    }
   ],
   "source": [
    "test_run='spicy meat salad'\n",
    "\n",
    "vec_bow = my_nlp_dict.doc2bow(test_run.split())\n",
    "#print(vec_bow) #this part works\n",
    "vec_tfidf=tfidf_model[vec_bow]\n",
    "#print(vec_tfidf) # you join it with the model, not the corpus\n",
    "vec_lsi = lsi_model[vec_tfidf] # convert the query to LSI space\n",
    "sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "#print(list(enumerate(sims))) # print (document_number, document_similarity) 2-tuples\n",
    "\n",
    "top_10=list(sorted(enumerate(sims), key=lambda x: x[1], reverse=True))[:20]\n",
    "top_10_indexes=[x[0] for x in top_10]\n",
    "top_10_match_pct=[x[1] for x in top_10]\n",
    "print(\"INDEXES\")\n",
    "print(top_10_indexes)\n",
    "top_10_names=x_train.name[top_10_indexes].values\n",
    "\n",
    "print(\"RECIPES\")\n",
    "pp.pprint(list(zip(top_10_names,top_10_match_pct)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the similarities overall have decreased. As the recipes are stretched across 50 topics, they are less and less similar to each other. But the overall ordering still seems good. Is this better than before? Hard to say. For one, there seems to be less mention of anything spicy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOW FOR LDA\n",
    "\n",
    "You specify the number of LDA topics here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using old LDA model\n"
     ]
    }
   ],
   "source": [
    "#trying with LDA first\n",
    "lda_num_topics=1000 #for lda it seems I need to increase the number of topics in order to make it discriminating enough\n",
    "\n",
    "try:\n",
    "    1/use_old_models # if use_old_models=0, then this fails\n",
    "    lda_model = gensim.models.LdaModel.load(os.path.join(data_folder,'lda_'+str(lda_num_topics)+'.model'))\n",
    "    \n",
    "    print(\"using old LDA model\")\n",
    "except:\n",
    "    print(\"CREATING LDA\")\n",
    "\n",
    "    print(\"creating LDA model\")\n",
    "    %time lda_model = gensim.models.LdaModel(corpus_tfidf, id2word=my_nlp_dict, num_topics=lda_num_topics)\n",
    "    lda_model.save(os.path.join(data_folder,'lda_'+str(lda_num_topics)+'.model'))\n",
    "    \n",
    "lda_corpus=lda_model[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# SIMILARITIES MATRIX WITH LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading similarities matrix\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    1/use_old_models \n",
    "    index = similarities.MatrixSimilarity.load(os.path.join(data_folder, 'lda_similarity_'+str(lda_num_topics)+'.index'))\n",
    "    print(\"loading similarities matrix\")\n",
    "except:\n",
    "    print(\"creating new similarities matrix\")\n",
    "    index = similarities.MatrixSimilarity(lda_corpus)\n",
    "    index.save(os.path.join(data_folder, 'lda_similarity_'+str(lda_num_topics)+'.index'))\n",
    "    index = similarities.MatrixSimilarity.load(os.path.join(data_folder, 'lda_similarity_'+str(lda_num_topics)+'.index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Can it recognize itself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target recipe at index 0\n",
      "Rumaki \n",
      "INDEXES\n",
      "[0, 1109, 20036, 3885, 68, 4430, 20679, 1635, 15772, 24372, 8869, 3868, 6886, 25384, 22863, 20572, 28358, 16554, 2687, 19084]\n",
      "RECIPES\n",
      "[   ('Rumaki ', 0.99997115),\n",
      "    ('Mou-Shou Pork ', 0.68599117),\n",
      "    ('BEEF KWANGTON #1 ', 0.68405294),\n",
      "    ('Beef Kwangton B1 ', 0.6430223),\n",
      "    ('Seven Happiness Soup ', 0.62071407),\n",
      "    ('Seven Happiness Soup ', 0.62055957),\n",
      "    ('Chinese Hamburger ', 0.61910391),\n",
      "    ('Kiwifruit Ice ', 0.60157776),\n",
      "    ('Chicken With Zucchini ', 0.59555656),\n",
      "    ('Hot & Sour Soup     -Kvnh17b ', 0.59267223),\n",
      "    ('Fire Pot ', 0.58878523),\n",
      "    ('Fire Pot ', 0.58748519),\n",
      "    ('Cantonese Egg Foo Yung ', 0.57591391),\n",
      "    ('Sukiyaki- [Weight Watchers New International Cookbook] ', 0.55940592),\n",
      "    ('Foo Yung Don (Egg Foo Yung) ', 0.5500977),\n",
      "    ('Chinese Pork-Lettuce Rolls ', 0.54797667),\n",
      "    ('Chinese: Spicy Chicken (Le Tze Gee) - Shangtu ', 0.52837127),\n",
      "    ('Hot & Sour Chicken Soup ', 0.52731884),\n",
      "    ('Golden Tiger Dumpling Soup ', 0.52726138),\n",
      "    ('Mandarin Hot And Sour Soup ', 0.51678431)]\n"
     ]
    }
   ],
   "source": [
    "test_run=x_train.corpus[0]\n",
    "\n",
    "vec_bow = my_nlp_dict.doc2bow(test_run.split())\n",
    "#print(vec_bow) #this part works\n",
    "vec_tfidf=tfidf_model[vec_bow]\n",
    "#print(vec_tfidf) # you join it with the model, not the corpus\n",
    "vec_lda = lda_model[vec_tfidf] # convert the query to LSI space\n",
    "sims = index[vec_lda] # perform a similarity query against the corpus\n",
    "#print(list(enumerate(sims))) # print (document_number, document_similarity) 2-tuples\n",
    "\n",
    "top_10=list(sorted(enumerate(sims), key=lambda x: x[1], reverse=True))[:20]\n",
    "top_10_indexes=[x[0] for x in top_10]\n",
    "top_10_match_pct=[x[1] for x in top_10]\n",
    "print(\"Target recipe at index 0\")\n",
    "print(x_train.name[0])\n",
    "print(\"INDEXES\")\n",
    "print(top_10_indexes)\n",
    "top_10_names=x_train.name[top_10_indexes].values\n",
    "\n",
    "print(\"RECIPES\")\n",
    "pp.pprint(list(zip(top_10_names,top_10_match_pct)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yes it can!\n",
    "\n",
    "Once again it can pick itself out. One thing of note though, it seems for LDA you need to specify a much higher number of topics than lsi for it to be discriminating enough. Too few number of topics and all the recipes look too alike. \n",
    "\n",
    "I specified 1000 topics but in reality that might be too much. If you look at the similarity percentages, the second suggestion has a massive drop off in similarity. That might mean that we've overfit the data. If thinking of the 'topics space' where one could see all the 1000 clusters of recipes, Rumaki might be the only one in his group. \n",
    "\n",
    "Feel free to play around with the number of topics and see for yourself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Making a request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEXES\n",
      "[24577, 1788, 14356, 1446, 24945, 10396, 4045, 20455, 21677, 12665, 17453, 27621, 6173, 12645, 26539, 20233, 10389, 7279, 9095, 4083]\n",
      "RECIPES\n",
      "[   ('Spicy Steak Marinade ', 0.64964396),\n",
      "    ('Spicy Steak Marinade ', 0.64833283),\n",
      "    ('Spicy Steak Marinade ', 0.64468384),\n",
      "    ('Chicken Cassandra ', 0.48927861),\n",
      "    ('Chicken Cassandra+++fggt98b ', 0.48921406),\n",
      "    (\"Klein's Kanchi Chicken \", 0.47572663),\n",
      "    ('Italian Slaw ', 0.4398284),\n",
      "    ('Sweet Salad Dressing Mix ', 0.4324199),\n",
      "    ('Easy Spicy Meat Loaf ', 0.43042564),\n",
      "    ('Good Reasons Italian Dressing ', 0.42516854),\n",
      "    ('Spicy Grilled Chicken Rbtn28a ', 0.42421454),\n",
      "    ('Spicy Fresh Country Sausage ', 0.41986454),\n",
      "    ('Dixie Boiled Salad Dressing ', 0.4115974),\n",
      "    ('Spicy Turkey Loaf ', 0.41049859),\n",
      "    ('Siberian Pelmeni (Meat-Filled Noodles With Va ', 0.40292296),\n",
      "    ('Spicy Breakfast Pattycakes ', 0.39681509),\n",
      "    ('Italian-Cheese Dressing ', 0.39315721),\n",
      "    ('Big Bucket In The Sky Chicken (Kfc) ', 0.39266184),\n",
      "    ('Salsa Chicken W/Brussels Sprouts ', 0.38718116),\n",
      "    ('Cream Cheese Chicken ', 0.38700309)]\n"
     ]
    }
   ],
   "source": [
    "test_run='spicy meat salad'\n",
    "\n",
    "vec_bow = my_nlp_dict.doc2bow(test_run.split())\n",
    "#print(vec_bow) #this part works\n",
    "vec_tfidf=tfidf_model[vec_bow]\n",
    "#print(vec_tfidf) # you join it with the model, not the corpus\n",
    "vec_lda = lda_model[vec_tfidf] # convert the query to LSI space\n",
    "sims = index[vec_lda] # perform a similarity query against the corpus\n",
    "#print(list(enumerate(sims))) # print (document_number, document_similarity) 2-tuples\n",
    "\n",
    "top_10=list(sorted(enumerate(sims), key=lambda x: x[1], reverse=True))[:20]\n",
    "top_10_indexes=[x[0] for x in top_10]\n",
    "top_10_match_pct=[x[1] for x in top_10]\n",
    "print(\"INDEXES\")\n",
    "print(top_10_indexes)\n",
    "top_10_names=x_train.name[top_10_indexes].values\n",
    "\n",
    "print(\"RECIPES\")\n",
    "pp.pprint(list(zip(top_10_names,top_10_match_pct)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately this doesn't look as good as with the LSI model. I can see that it capture the spicy meat part. \n",
    "\n",
    "Let's try again with fewer topics..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which topic does it resemble most? What words best represent those topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic numbers are\n",
      "(51, 539, 723)\n",
      "----\n",
      "0.664*\"spicy\" + 0.058*\"ground\" + 0.048*\"black\" + 0.046*\"crush\" + 0.034*\"clove\" + 0.031*\"fresh\" + 0.020*\"sauce\" + 0.019*\"coarsely\" + 0.017*\"steak\" + 0.012*\"cut\"\n",
      "----\n",
      "0.306*\"salad\" + 0.233*\"italian\" + 0.203*\"dress\" + 0.047*\"fggt98b\" + 0.037*\"tomato\" + 0.030*\"wine\" + 0.028*\"season\" + 0.020*\"chicken\" + 0.017*\"pkg\" + 0.016*\"white\"\n",
      "----\n",
      "0.699*\"meat\" + 0.079*\"cook\" + 0.041*\"beef\" + 0.037*\"tomato\" + 0.033*\"crush\" + 0.031*\"celery\" + 0.027*\"hot\" + 0.011*\"single\" + 0.008*\"ginger\" + 0.003*\"16\"\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "check_topics_words(lda_model,vec_lda,nber_words=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And finally now for Doc2Vec\n",
    "\n",
    "## What is Doc2Vec?\n",
    "\n",
    "It's a way of turning documents into dense matrices with the help of neural networks. The original paper can be found here https://arxiv.org/pdf/1405.4053v2.pdf\n",
    "\n",
    "I won't really be going over the theory in detail, but know that there are two version of Doc2Vec. \n",
    "\n",
    "One is based on the continuous bag of words model (Give the model a context and it will give you a word)\n",
    "\n",
    "While the other version is more akin to the skip gram model (Give me a word and I will give you a context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Based on Continuous Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtkAAAGLCAIAAACdgRmoAAAAAXNSR0IArs4c6QAAAARnQU1BAACx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\nYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=os.path.join(data_folder,'doc2vec_cbow.png')) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key thing to keep in mind is that the words of all the articles are trained and formed by the same matrix, for all articles. It is trained very similarly to a normal CBOW with a sliding window. In the above example you feed it ['the','cat','sat'], we turn these into their BOW vectors and send them through a neural net layer to predict \"on\" in this case. In the next article, we could have ['the','cat','sat'] --> in. The model will have to generalize.\n",
    "\n",
    "Now on top of that for every sliding window input you will have the paragraph (or document) id. This is either averaged or concatenated in just as if it was another word in the sentence. This document id is the TAG. The tag is whatever we want to identify, group the document by. It could be just the name of the article (like in our case, the name of the recipes) or it could be a binary \"good\" or \"bad\" which you attribute to all your documents (think sentiment analysis for movie reviews).\n",
    "\n",
    "There are two phases to the traning, one where both the paragraph id and vocabulary are trained at the same time, followed by phase where the weights for the vocabulary stays fixed ona\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Doc2Vec corpus\n",
    "\n",
    "def read_corpus(df,tokens_only=False): \n",
    "    #print(df.head())\n",
    "    for line in df.itertuples():\n",
    "        text_name=line[2]\n",
    "        text_input=line[1]\n",
    "\n",
    "        if tokens_only:\n",
    "            #return df.corpus.tolist()\n",
    "            yield text_input\n",
    "            #yield gensim.utils.simple_preprocess(line)\n",
    "\n",
    "        else:\n",
    "            #yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line),[i])\n",
    "            #return gensim.models.doc2vec.TaggedDocument(df.corpus.tolist(),df.ID.tolist())\n",
    "            yield gensim.models.doc2vec.TaggedDocument([text_input],[text_name])\n",
    "            #the i is the tag in this case\n",
    "            #notice here that you're giving back a MODEL\n",
    "            #tagged document only works 1 document at time\n",
    "            \n",
    "train_corpus=list(read_corpus(x_train))\n",
    "test_corpus=list(read_corpus(x_test,tokens_only=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checking to see how many words I want to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 0, size of vocab:  19707.85714285714\n",
      "min_count: 1, size of vocab:  19707.85714285714\n",
      "min_count: 2, size of vocab:  1417.857142857143\n",
      "min_count: 3, size of vocab:  70.0\n",
      "min_count: 4, size of vocab:  20.0\n",
      "min_count: 5, size of vocab:  3.5714285714285716\n",
      "min_count: 6, size of vocab:  0.0\n",
      "min_count: 7, size of vocab:  0.0\n",
      "min_count: 8, size of vocab:  0.0\n",
      "min_count: 9, size of vocab:  0.0\n",
      "min_count: 10, size of vocab:  0.0\n",
      "min_count: 11, size of vocab:  0.0\n",
      "min_count: 12, size of vocab:  0.0\n",
      "min_count: 13, size of vocab:  0.0\n",
      "min_count: 14, size of vocab:  0.0\n",
      "min_count: 15, size of vocab:  0.0\n",
      "min_count: 16, size of vocab:  0.0\n",
      "min_count: 17, size of vocab:  0.0\n",
      "min_count: 18, size of vocab:  0.0\n",
      "min_count: 19, size of vocab:  0.0\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
    "pre = Doc2Vec(min_count=0)\n",
    "pre.scan_vocab(train_corpus) \n",
    "\n",
    "for num in range(0, 20):\n",
    "    print('min_count: {}, size of vocab: '.format(num), pre.scale_vocab(min_count=num, dry_run=True)['memory']['vocab']/700)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#CHOOSE YOUR MIN COUNT HERE\n",
    "MIN_COUNT=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow,d300,n5,s0.001,t4)\n",
      "Doc2Vec(dbow,d600,n5,s0.001,t4)\n",
      "Doc2Vec(dbow,d600,n5,s0.001,t4)\n",
      "Doc2Vec(dm/m,d300,n5,w8,s0.001,t4)\n",
      "Doc2Vec(dm/m,d300,n5,w4,s0.001,t4)\n",
      "Doc2Vec(dm/m,d600,n5,w8,s0.001,t4)\n",
      "Doc2Vec(dm/m,d600,n5,w4,s0.001,t4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#dm=1  #means distributed memory method which is the default\n",
    "#dm=0 # means dbow method\n",
    "starting_alpha=0.025\n",
    "\n",
    "D2V_models = [\n",
    "    \n",
    "    # PV-DBOW \n",
    "    #Doc2Vec(dm=0, dbow_words=1, size=20, window=2,alpha=starting_alpha, min_alpha=starting_alpha, min_count=MIN_COUNT, workers=cores,iter=1),\n",
    "    #Doc2Vec(dm=0, dbow_words=1, size=20, window=4,alpha=starting_alpha, min_alpha=starting_alpha, min_count=MIN_COUNT, workers=cores,iter=1),\n",
    "    # PV-DBOW \n",
    "    #Doc2Vec(dm=0, dbow_words=1, size=20, window=4,alpha=starting_alpha, min_alpha=starting_alpha, min_count=MIN_COUNT, workers=cores,iter=1),\n",
    "    #Doc2Vec(dm=0, dbow_words=1, size=20, window=20,alpha=starting_alpha, min_alpha=starting_alpha, min_count=MIN_COUNT, workers=cores,iter=1),\n",
    "    # PV-DM \n",
    "    #Doc2Vec(dm=1, dm_mean=1, size=20, window=2,alpha=starting_alpha, min_alpha=starting_alpha, min_count=MIN_COUNT, workers=cores,iter=1),\n",
    "    #Doc2Vec(dm=1, dm_mean=1, size=20, window=10,alpha=starting_alpha, min_alpha=starting_alpha, min_count=MIN_COUNT, workers=cores,iter=1),\n",
    "    # PV-DM \n",
    "    #Doc2Vec(dm=1, dm_mean=1, size=20, window=4,alpha=starting_alpha, min_alpha=starting_alpha, min_count=MIN_COUNT, workers=cores,iter=1),\n",
    "    #Doc2Vec(dm=1, dm_mean=1, size=20, window=20,alpha=starting_alpha, min_alpha=starting_alpha, min_count=MIN_COUNT, workers=cores,iter=1),\n",
    "    \n",
    "    \n",
    "    #these are bad\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=300, window=8,alpha=starting_alpha, min_alpha=starting_alpha, min_count=MIN_COUNT, workers=cores,iter=1),\n",
    "    Doc2Vec(dm=0, size=300, window=4,alpha=starting_alpha, min_alpha=starting_alpha, min_count=MIN_COUNT, workers=cores,iter=1),\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=600, window=8,alpha=starting_alpha, min_alpha=starting_alpha, min_count=MIN_COUNT, workers=cores,iter=1),\n",
    "    Doc2Vec(dm=0, size=600, window=4,alpha=starting_alpha, min_alpha=starting_alpha, min_count=MIN_COUNT, workers=cores,iter=1),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, size=300, window=8,alpha=starting_alpha, min_alpha=starting_alpha, min_count=MIN_COUNT, workers=cores,iter=1),\n",
    "    Doc2Vec(dm=1, size=300, window=4,alpha=starting_alpha, min_alpha=starting_alpha, min_count=MIN_COUNT, workers=cores,iter=1),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, size=600, window=8,alpha=starting_alpha, min_alpha=starting_alpha, min_count=MIN_COUNT, workers=cores,iter=1),\n",
    "    Doc2Vec(dm=1, size=600, window=4,alpha=starting_alpha, min_alpha=starting_alpha, min_count=MIN_COUNT, workers=cores,iter=1),\n",
    "    \n",
    "]\n",
    "\n",
    "D2V_models[0].build_vocab(train_corpus) # \n",
    "for model in D2V_models[1:]:\n",
    "    model.reset_from(D2V_models[0])\n",
    "    print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "models_by_name = OrderedDict((str(model).replace(\"/\",\"_\").replace(\",\",\"_\").replace(\".\",\"_\"), model) for model in D2V_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model\n",
      "NOW AT EPOCH 0\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:44,614: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:45,085: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:45,645: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:46,212: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:46,786: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:47,385: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 1\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:47,880: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:48,357: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:48,907: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:49,451: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:50,007: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:50,559: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 2\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:51,056: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:51,546: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:52,129: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:52,682: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:53,245: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:53,855: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 3\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:54,325: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:54,801: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:55,359: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:55,914: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:56,504: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:57,082: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 4\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:57,552: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:58,031: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:58,629: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:59,214: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:16:59,818: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:00,435: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 5\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:00,924: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:01,403: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:01,957: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:02,515: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:03,065: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:03,620: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 6\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:04,113: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:04,590: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:05,143: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:05,696: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:06,284: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:06,873: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 7\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:07,357: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:07,827: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:08,376: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:08,923: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:09,467: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:10,020: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 8\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:10,504: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:10,977: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:11,526: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:12,080: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:12,651: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:13,201: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 9\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:13,700: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:14,184: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:14,732: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:15,280: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:15,831: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:16,382: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 10\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:16,869: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:17,348: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:17,903: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:18,470: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:19,074: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:19,663: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 11\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:20,144: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:20,617: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:21,161: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:21,703: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:22,244: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:22,795: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 12\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:23,279: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:23,736: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:24,281: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:24,832: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:25,408: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:25,998: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 13\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:26,482: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:26,958: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:27,540: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:28,090: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:28,672: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:29,226: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 14\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:29,722: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:30,199: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:30,763: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:31,319: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:31,878: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:32,441: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 15\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:32,919: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:33,391: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:33,971: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:34,533: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:35,083: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:35,648: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 16\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:36,140: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:36,615: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:37,160: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:37,713: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:38,271: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:38,821: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 17\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:39,308: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:39,788: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:40,337: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:40,892: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:41,435: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:41,995: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 18\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:42,491: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:42,965: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:43,512: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:44,063: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:44,620: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:45,172: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 19\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:45,658: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:46,132: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:46,683: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:47,232: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:47,780: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:48,337: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 20\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:48,832: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:49,300: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:49,846: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:50,395: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:50,949: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:51,515: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 21\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:51,999: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:52,463: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:53,016: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:53,573: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:54,119: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:54,680: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 22\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:55,164: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:55,638: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:56,184: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:56,741: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:57,304: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:57,850: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 23\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:58,334: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:58,812: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:59,362: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:17:59,913: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:00,463: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:01,010: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 24\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:01,503: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:01,974: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:02,523: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:03,080: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:03,637: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:04,192: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 25\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:04,682: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:05,152: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:05,701: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:06,257: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:06,805: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:07,371: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 26\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:07,850: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:08,326: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:08,872: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:09,427: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:09,984: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:10,528: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 27\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:11,009: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:11,475: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:12,018: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:12,564: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:13,105: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:13,693: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 28\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:14,172: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:14,643: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:15,186: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:15,741: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:16,288: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:16,834: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 29\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:17,313: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:17,783: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:18,330: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:18,872: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:19,415: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:19,968: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 30\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:20,447: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:20,917: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:21,464: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:22,001: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:22,546: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:23,085: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 31\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:23,566: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:24,029: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:24,574: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:25,110: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:25,649: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:26,191: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 32\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:26,675: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:27,142: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:27,673: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:28,212: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:28,755: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:29,288: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 33\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:29,768: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:30,236: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:30,772: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:31,308: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:31,846: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:32,391: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 34\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:32,867: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:33,338: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:33,873: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:34,412: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:34,952: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:35,499: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 35\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:35,980: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:36,451: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:37,017: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:37,556: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:38,089: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:38,635: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 36\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:39,117: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:39,584: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:40,121: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:40,657: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:41,201: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:41,746: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 37\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:42,224: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:42,693: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:43,232: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:43,773: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:44,312: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:44,910: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 38\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:45,383: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:45,850: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:46,387: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:46,959: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:47,547: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:48,086: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 39\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:48,564: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:49,031: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:49,574: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:50,128: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:50,707: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:51,289: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 40\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:51,764: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:52,272: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:52,909: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:53,503: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:54,100: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:54,689: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 41\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:55,161: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:55,629: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:56,164: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:56,707: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:57,244: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:57,789: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 42\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:58,261: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:58,730: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:59,269: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:18:59,811: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:00,360: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:00,960: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 43\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:01,439: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:01,910: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:02,455: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:02,993: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:03,539: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:04,085: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 44\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:04,567: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:05,035: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:05,569: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:06,110: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:06,655: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:07,196: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 45\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:07,672: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:08,139: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:08,682: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:09,224: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:09,765: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:10,309: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 46\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:10,793: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:11,260: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:11,799: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:12,333: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:12,873: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:13,419: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 47\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:13,897: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:14,363: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:14,902: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:15,446: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:15,986: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:16,535: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 48\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:17,017: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:17,488: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:18,024: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:18,564: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:19,106: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:19,648: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 49\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:20,125: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:20,593: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:21,127: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:21,670: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:22,213: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:22,761: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 50\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:23,240: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:23,711: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:24,248: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:24,787: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:25,325: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:25,866: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 51\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:26,347: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:26,810: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:27,390: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:27,967: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:28,517: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:29,061: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 52\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:29,540: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:30,005: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:30,546: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:31,085: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:31,631: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:32,172: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 53\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:32,669: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:33,136: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:33,679: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:34,216: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:34,754: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:35,306: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 54\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:35,781: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:36,248: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:36,787: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:37,325: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:37,865: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:38,410: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 55\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:38,890: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:39,362: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:39,902: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:40,442: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:40,978: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:41,525: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 56\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:42,000: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:42,471: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:43,008: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:43,551: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:44,094: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:44,637: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 57\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:45,118: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:45,585: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:46,129: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:46,665: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:47,201: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:47,748: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 58\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:48,220: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:48,688: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:49,220: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:49,761: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:50,305: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:50,853: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 59\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:51,332: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:51,802: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:52,345: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:52,886: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:53,427: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:53,967: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 60\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:54,451: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:54,915: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:55,456: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:55,991: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:56,531: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:57,078: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 61\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:57,556: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:58,023: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:58,563: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:59,096: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:19:59,638: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:00,179: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 62\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:00,654: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:01,128: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:01,673: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:02,257: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:02,839: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:03,395: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 63\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:03,881: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:04,358: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:04,923: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:05,483: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:06,033: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:06,607: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 64\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:07,094: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:07,567: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:08,114: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:08,666: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:09,224: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:09,782: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 65\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:10,272: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:10,757: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:11,304: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:11,850: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:12,398: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:12,947: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 66\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:13,435: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:13,897: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:14,443: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:14,986: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:15,576: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:16,168: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 67\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:16,652: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:17,128: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:17,678: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:18,226: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:18,776: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:19,329: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 68\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:19,821: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:20,297: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:20,843: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:21,399: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:21,962: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:22,523: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 69\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:23,007: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:23,483: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:24,037: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:24,592: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:25,148: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:25,707: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 70\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:26,198: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:26,669: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:27,220: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:27,778: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:28,331: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:28,888: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 71\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:29,378: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:29,857: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:30,415: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:30,968: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:31,518: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:32,075: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 72\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:32,563: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:33,036: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:33,579: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:34,120: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:34,666: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:35,224: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 73\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:35,707: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:36,182: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:36,769: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:37,325: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:37,879: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:38,435: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 74\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:38,914: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:39,387: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:39,934: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:40,470: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:41,010: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:41,559: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 75\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:42,045: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:42,519: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:43,075: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:43,628: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:44,175: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:44,770: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 76\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:45,251: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:45,728: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:46,264: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:46,808: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:47,353: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:47,896: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 77\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:48,375: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:48,846: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:49,387: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:49,927: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:50,467: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:51,021: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 78\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:51,499: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:51,965: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:52,508: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:53,051: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:53,592: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:54,132: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 79\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:54,608: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:55,080: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:55,616: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:56,163: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:56,701: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:57,240: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 80\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:57,716: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:58,183: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:58,721: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:59,260: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:20:59,802: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:00,349: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 81\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:00,837: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:01,300: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:01,842: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:02,379: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:02,917: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:03,456: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 82\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:03,938: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:04,403: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:04,943: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:05,482: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:06,023: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:06,568: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 83\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:07,046: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:07,515: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:08,056: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:08,592: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:09,131: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:09,672: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 84\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:10,156: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:10,625: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:11,164: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:11,700: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:12,245: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:12,788: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 85\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:13,268: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:13,733: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:14,268: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:14,809: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:15,349: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:15,893: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 86\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:16,375: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:16,841: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:17,375: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:17,915: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:18,488: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:19,036: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 87\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:19,515: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:19,981: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:20,524: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:21,060: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:21,604: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:22,152: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 88\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:22,634: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:23,099: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:23,639: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:24,184: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:24,730: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:25,273: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 89\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:25,750: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:26,220: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:26,756: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:27,299: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:27,836: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:28,383: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 90\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:28,868: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:29,337: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:29,876: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:30,419: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:30,964: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:31,508: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 91\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:31,983: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:32,455: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:33,013: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:33,551: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:34,093: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:34,644: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 92\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:35,129: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:35,595: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:36,132: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:36,674: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:37,216: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:37,760: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 93\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:38,244: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:38,725: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:39,320: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:39,864: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:40,400: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:40,945: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 94\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:41,423: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:41,897: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:42,431: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:42,968: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:43,511: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:44,057: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 95\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:44,535: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:45,002: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:45,547: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:46,087: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:46,623: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:47,168: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 96\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:47,650: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:48,118: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:48,652: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:49,188: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:49,736: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:50,276: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 97\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:50,755: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:51,220: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:51,760: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:52,297: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:52,841: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:53,379: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 98\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:53,852: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:54,316: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:54,858: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:55,395: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:55,939: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:56,483: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW AT EPOCH 99\n",
      "training Doc2Vec(dbow_d300_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:56,966: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dbow_d600_n5_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:57,427: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:57,969: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:58,511: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:59,050: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-18 15:21:59,591: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    1/use_old_models # if use_old_models=0, then this fails\n",
    "\n",
    "    for name, train_model in models_by_name.items():\n",
    "        train_model = gensim.models.Doc2Vec.load(os.path.join(data_folder,name+'.mm'))\n",
    "\n",
    "    print(\"loading model\")\n",
    "except:\n",
    "    from random import shuffle\n",
    "    print(\"training model\")\n",
    "    epochs=100\n",
    "    \n",
    "    last_alpha=0.001\n",
    "    alpha_step=(starting_alpha-last_alpha)/epochs\n",
    "    \n",
    "\n",
    "    alpha=starting_alpha\n",
    "        \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"NOW AT EPOCH %i\"%epoch)\n",
    "        shuffle(train_corpus) #shuffling causes a problem for some reason\n",
    "        \n",
    "        for name, train_model in models_by_name.items():\n",
    "            print(\"training \"+name)\n",
    "            train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "            train_model.train(train_corpus, total_examples=len(train_corpus), epochs=1)    \n",
    "        \n",
    "        alpha-=alpha_step\n",
    "\n",
    "    #when you train your vocabulary, make sure your articles look like =[['word1','word2',...]] and not['word1 word2']\n",
    "    #D2V_model = gensim.models.doc2vec.Doc2Vec(size=1000, window=10,min_count=0, iter=55)\n",
    "    #D2V_model.build_vocab(train_corpus)\n",
    "    #%time D2V_model.train(train_corpus,total_examples=D2V_model.corpus_count, epochs=D2V_model.iter)\n",
    "    #D2V_model.save(os.path.join(data_folder,'recipes_d2v.mm'))\n",
    "    print(\"done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# can it recognize itself?\n",
    "\n",
    "Unfortunately, after all this, it doesn't seem to do as good a job as the previous methods seen before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow_d300_n5_s0_001_t4)\n",
      "guesses were\n",
      "[('Chocolate Fantasy Cheesecake ', 0.2311915159225464), ('Individual Cherry Cheesecake ', 0.21800042688846588), ('Grilled Prawn W/Tequila & Nectarine Cream Sauce ', 0.20166105031967163), ('Low-Cal Brownies ', 0.19787496328353882), ('Steamed Fish With Ginger ', 0.1972135305404663), ('Spiced Cornbread ', 0.1964753270149231), ('Strawberry Shortcake Squares ', 0.1961410939693451), ('Corn Chowder(C) (Lacto) ', 0.1952270120382309), (\"Grandma Weiss's Kimmel Soup \", 0.1939014196395874), ('Chinese Beef Jerky ', 0.193346306681633)]\n",
      "answer was \n",
      "['Salmon Sushi ']\n",
      "----\n",
      "Doc2Vec(dbow_d600_n5_s0_001_t4)\n",
      "guesses were\n",
      "[('Spiced Cornbread ', 0.18554265797138214), ('Steamed Fish With Ginger & Onions ', 0.16951081156730652), ('Chocolate Fantasy Cheesecake ', 0.15151645243167877), ('Lemon Tea Bread ', 0.14163050055503845), ('Pork Loin In Mustard Sauce ', 0.14092785120010376), ('Goat Soup ', 0.14019480347633362), ('Keitetyt Ravut (Dill-Flavored Crayfish) ', 0.13773083686828613), ('York County Farm Apple Mystery Pudding ', 0.13730140030384064), ('Champeach Sorbet ', 0.13722047209739685), ('Vegetable Soup With Lentils (Vegan) ', 0.13587678968906403)]\n",
      "answer was \n",
      "['Salmon Sushi ']\n",
      "----\n",
      "Doc2Vec(dm_m_d300_n5_w8_s0_001_t4)\n",
      "guesses were\n",
      "[('Chocolate Fantasy Cheesecake ', 0.2513207495212555), ('Grilled Prawn W/Tequila & Nectarine Cream Sauce ', 0.2159629911184311), (\"Grandma Weiss's Kimmel Soup \", 0.2042531818151474), ('Baked Pork Chops & Corn ', 0.20339922606945038), ('Basic Cheesecake Method & Toppings ', 0.20240211486816406), (\"World's Second Health Cookie \", 0.2018788456916809), ('Macadamia Nut Fudge ', 0.2007279396057129), ('Bavarian Apple Cheesecake * ', 0.1990509331226349), ('Olive Dip ', 0.19753187894821167), ('Chinese Beef Jerky ', 0.1934734582901001)]\n",
      "answer was \n",
      "['Salmon Sushi ']\n",
      "----\n",
      "Doc2Vec(dm_m_d300_n5_w4_s0_001_t4)\n",
      "guesses were\n",
      "[('Chocolate Fantasy Cheesecake ', 0.2495361566543579), ('Grilled Prawn W/Tequila & Nectarine Cream Sauce ', 0.2201363891363144), (\"World's Second Health Cookie \", 0.2153380662202835), ('Baked Pork Chops & Corn ', 0.20721343159675598), ('Basic Cheesecake Method & Toppings ', 0.20642468333244324), ('Shrimp Egg Foo Yung ', 0.2048056721687317), ('Macadamia Nut Fudge ', 0.2017381191253662), ('Bavarian Apple Cheesecake * ', 0.19866034388542175), ('Individual Cherry Cheesecake ', 0.1970779448747635), ('Chinese Beef Jerky ', 0.1963471621274948)]\n",
      "answer was \n",
      "['Salmon Sushi ']\n",
      "----\n",
      "Doc2Vec(dm_m_d600_n5_w8_s0_001_t4)\n",
      "guesses were\n",
      "[('Steamed Fish With Ginger & Onions ', 0.16635948419570923), ('Pork Loin In Mustard Sauce ', 0.16201436519622803), ('Spiced Cornbread ', 0.16094523668289185), ('Chocolate Fantasy Cheesecake ', 0.1552196890115738), ('Lemon Tea Bread ', 0.1437273621559143), ('Goat Soup ', 0.14127643406391144), ('Vegetable Soup With Lentils (Vegan) ', 0.13871265947818756), ('York County Farm Apple Mystery Pudding ', 0.1384972333908081), ('King Ranch Chicken 2 ', 0.13819193840026855), ('Saute Of Shrimp With Bergamot Infusion ', 0.13641881942749023)]\n",
      "answer was \n",
      "['Salmon Sushi ']\n",
      "----\n",
      "Doc2Vec(dm_m_d600_n5_w4_s0_001_t4)\n",
      "guesses were\n",
      "[('Steamed Fish With Ginger & Onions ', 0.17003394663333893), ('Pork Loin In Mustard Sauce ', 0.16508862376213074), ('Spiced Cornbread ', 0.162961944937706), ('Chocolate Fantasy Cheesecake ', 0.15664762258529663), ('Vegetable Soup With Lentils (Vegan) ', 0.1499980390071869), ('Holiday Cranberry Rolls Ada   *Fhmn87a* ', 0.14284157752990723), ('Lemon Tea Bread ', 0.13983988761901855), ('Old-Fashioned Currant Tart ', 0.13861632347106934), ('Goat Soup ', 0.13803043961524963), ('Cake Mix Cookies ', 0.13597328960895538)]\n",
      "answer was \n",
      "['Salmon Sushi ']\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for name, train_model in models_by_name.items():\n",
    "\n",
    "    print(name)\n",
    "    inferred_vector=train_model.infer_vector(train_corpus[0].words)\n",
    "    sims=train_model.docvecs.most_similar([inferred_vector],topn=10)\n",
    "    print(\"guesses were\")\n",
    "    print(sims)\n",
    "    print(\"answer was \")\n",
    "    print(train_corpus[0].tags)\n",
    "    print(\"----\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# save these models? (only if they're good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#for name, train_model in models_by_name.items():\n",
    "#    train_model.save(os.path.join(data_folder,name+'.mm'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Assessing your model\n",
    "\n",
    "See how many of these docs can it properly identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-d65e6bdeda81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m#gives back an ARRAY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m#calling model.docvecs summons all the vecs of the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0msims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minferred_vector\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumber_of_similarities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;31m#sims gives the top 20 similar vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m#sims is a list of two variables -> docid and sim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ak\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, indexer)\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtopn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m         \u001b[1;31m# ignore (don't return) docs from the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 478\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_to_doctag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msim\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mclip_start\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdists\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msim\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbest\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msim\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mclip_start\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_docs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtopn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ak\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtopn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m         \u001b[1;31m# ignore (don't return) docs from the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 478\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_to_doctag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msim\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mclip_start\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdists\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msim\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbest\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msim\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mclip_start\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_docs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtopn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ak\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mindex_to_doctag\u001b[1;34m(self, i_index)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_to_doctag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mindex_to_doctag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m         \u001b[1;34m\"\"\"Return string key for given i_index, if available. Otherwise return raw int doctag (same int).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[0mcandidate_offset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi_index\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_rawint\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_collections=[]\n",
    "number_of_similarities=len(train_corpus)\n",
    "\n",
    "number_of_docs=len(train_corpus)\n",
    "for name, train_model in models_by_name.items():\n",
    "    \n",
    "    ranks=[]\n",
    "    second_rank=[]\n",
    "    for index,tag_object in enumerate(train_corpus): \n",
    "        doc_id=tag_object[1][0]#note that doc_id is your TAG\n",
    "        inferred_vector=train_model.infer_vector(train_corpus[index].words)\n",
    "        #gives back an ARRAY\n",
    "        #calling model.docvecs summons all the vecs of the model\n",
    "        sims=train_model.docvecs.most_similar([inferred_vector],topn=number_of_similarities)\n",
    "        #sims gives the top 20 similar vectors\n",
    "        #sims is a list of two variables -> docid and sim\n",
    "        #so below you first do for docid, sim in sims and then you only collect the docid\n",
    "        #you basically collect all the ids of the docs that were most similar\n",
    "        #and then you look for your original doc_id by index.\n",
    "        #if it's at 0, then it means that it guessed correctly.\n",
    "        #the correct article was it its best prediction\n",
    "        rank=[docid for docid, sim in sims].index(doc_id)\n",
    "        ranks.append(rank)\n",
    "        second_rank.append(sims[1]) #We do this so that we can see what was in second position\n",
    "    #print(name)\n",
    "    #print(collections.Counter(ranks))\n",
    "    all_collections.append((name,collections.Counter(ranks)))\n",
    "\n",
    "    #rank=[tag_id for tag_id, sim in sims].index(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "number_of_similarities=20\n",
    "#plt.rcParams['figure.figsize'] = (15, 15)\n",
    "\n",
    "number_of_docs=len(train_corpus)\n",
    "acc_percentages={}\n",
    "\n",
    "\n",
    "nber_of_graphs=len(all_collections)\n",
    "graphs_per_row=2\n",
    "nber_of_rows= np.ceil(nber_of_graphs/graphs_per_row)\n",
    "\n",
    "nber_of_rows_number=nber_of_rows*100\n",
    "graphs_per_row_number=graphs_per_row*10\n",
    "\n",
    "i=1\n",
    "base_number=nber_of_rows_number+graphs_per_row_number\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 3*nber_of_rows)\n",
    "#['revenueDifferencedRollingStandardization','revenueRollingNormalization','revenueDifferencedRollingNormalization']:\n",
    "    \n",
    "for each_column in bar_data.columns:    \n",
    "\n",
    "    plt.subplot(base_number+i) \n",
    "    labels, values = zip(*counter.items())\n",
    "    indexes = np.arange(len(labels))\n",
    "    width = 1\n",
    "    \n",
    "    acc_percentages[name]=counter[0]/number_of_docs\n",
    "    \n",
    "    plt.bar(indexes, values, width)\n",
    "    plt.xticks(indexes + width * 0.5, labels)\n",
    "    \n",
    "    plt.title(name)\n",
    "    i+=1\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_collections[0]\n",
    "\n",
    "#pprint(all_collections)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "number_of_similarities=len(train_corpus)\n",
    "number_of_docs=len(train_corpus)\n",
    "acc_percentages={}\n",
    "\n",
    "nber_of_graphs=len(all_collections)\n",
    "graphs_per_row=2\n",
    "nber_of_rows= np.ceil(nber_of_graphs/graphs_per_row)\n",
    "\n",
    "nber_of_rows_number=nber_of_rows*100\n",
    "graphs_per_row_number=graphs_per_row*10\n",
    "\n",
    "i=1\n",
    "base_number=nber_of_rows_number+graphs_per_row_number\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 3*nber_of_rows)\n",
    "\n",
    "\n",
    "for each_collection in all_collections:\n",
    "    name=each_collection[0]\n",
    "    counter=each_collection[1]\n",
    "    \n",
    "    plot_location=base_number+i\n",
    "    plt.subplot(plot_location)\n",
    "\n",
    "    labels, values = zip(*counter.items())\n",
    "    indexes = np.arange(len(labels))\n",
    "    width = 1\n",
    "\n",
    "    acc_percentages[name]=counter[0]/number_of_docs\n",
    "    plt.bar(indexes, values, width)\n",
    "    plt.xticks(indexes + width * 0.5, labels)\n",
    "\n",
    "    plt.title(name)\n",
    "    i+=1\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# looking at their accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "acc_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import operator\n",
    "winning_model_name = sorted(acc_percentages.items(), key=operator.itemgetter(0))[0][0]\n",
    "winning_model=models_by_name[winning_model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# looking at most similar, median and most dissimilar texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "doc_index=0\n",
    "\n",
    "inferred_vector=winning_model.infer_vector(train_corpus[doc_index].words)\n",
    "\n",
    "sims=winning_model.docvecs.most_similar([inferred_vector],topn=len(winning_model.docvecs))\n",
    "\n",
    "\n",
    "print('Document ({}): {}\\n'.format(doc_index, ' '.join(train_corpus[doc_index].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % winning_model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: %s\\n' % (label, sims[index], ' '.join(train_corpus[index].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus))\n",
    "inferred_vector = winning_model.infer_vector(test_corpus[doc_id])\n",
    "sims = winning_model.docvecs.most_similar([inferred_vector], topn=len(winning_model.docvecs))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): {}\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % winning_model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: %s\\n' % (label, sims[index], ' '.join(train_corpus[index].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Talk to the chef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "request=text_process(\"I want a meat pie with a mexican twist\")\n",
    "\n",
    "#what I want here is a name \n",
    "#it's not necessarily most similar that I want, I'm looking to add the words to to each other. I think.\n",
    "\n",
    "#need an lstm for this? hopefully not. \n",
    "\n",
    "answers=winning_model.most_similar(positive=request, negative=[])\n",
    "\n",
    "print(answers)\n",
    "\n",
    "print(\"suggested meal\")\n",
    "answer=D2V_model.most_similar(positive=[D2V_model.infer_vector(request)], negative=[], topn=1)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# WORD2VEC\n",
    "\n",
    "\n",
    "try:\n",
    "    1/use_old_models # if use_old_models=0, then this fails\n",
    "    W2V_model = gensim.models.Word2Vec.load(os.path.join(data_folder,'recipes_w2v.mm'))\n",
    "    print(\"loading model\")\n",
    "except:\n",
    "    print(\"training model\")\n",
    "\n",
    "    \n",
    "    #Word2Vec model must take in [['text1word1,'text1word2'],['text2word1',...]\n",
    "    W2V_model=gensim.models.Word2Vec(train_processed_tokenized_text,min_count=1)\n",
    "    #the in_count=1 IS SO IMPORTANT. LOST 1 hour because of that. \n",
    "    W2V_model.save(os.path.join(data_folder,'recipes_w2v.mm'))\n",
    "    \n",
    "W2V_model.similarity('bread', 'pasta')\n",
    "W2V_model.most_similar(positive=['bread','meat'],negative=[\"salad\"],topn=2)\n",
    "W2V_model.similarity('water', 'juice')\n",
    "W2V_model.similarity('beef', 'pork')\n",
    "W2V_model.similarity('lb', 'oz')\n",
    "W2V_model.similarity('beef', 'chicken')\n",
    "W2V_model.similarity('oil', 'butter')\n",
    "\n",
    "\n",
    "\n",
    "# Talk to the chef\n",
    "\n",
    "request=text_process(\"I want a meat pie with a mexican twist\")\n",
    "\n",
    "#what I want here is a name \n",
    "#it's not necessarily most similar that I want, I'm looking to add the words to to each other. I think.\n",
    "\n",
    "#need an lstm for this? hopefully not. \n",
    "\n",
    "answers=W2V_model.most_similar(positive=request, negative=[])\n",
    "\n",
    "print(answers)\n",
    "\n",
    "print(\"suggested meal\")\n",
    "answer=D2V_model.most_similar(positive=[D2V_model.infer_vector(request)], negative=[], topn=1)\n",
    "print(answer)\n",
    "\n",
    "# can it recognize itself?\n",
    "\n",
    "#D2V_model.infer_vector(D2V_model)\n",
    "inferred_vector=D2V_model.infer_vector(train_corpus[0].words)\n",
    "sims=D2V_model.docvecs.most_similar([inferred_vector],topn=1)\n",
    "\n",
    "print(sims)\n",
    "print(train_corpus[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
